# AUM Scraper Backend

Sistema backend para coleta automatizada de Patrim√¥nio Sob Gest√£o (AUM) de empresas do mercado financeiro atrav√©s de scraping web e extra√ß√£o via GPT-4o.

## üöÄ Funcionalidades

- **Scraping Web Inteligente**: Coleta dados de sites, LinkedIn, Instagram e Twitter/X
- **Extra√ß√£o via IA**: Usa GPT-4o para extrair informa√ß√µes de AUM do conte√∫do coletado
- **Controle de Budget**: Monitora e limita gastos da API OpenAI
- **Processamento em Lote**: Processa m√∫ltiplas empresas de forma eficiente
- **Exporta√ß√£o Excel**: Gera relat√≥rios completos em formato Excel
- **API REST Completa**: Endpoints documentados com Swagger/OpenAPI
- **Hist√≥rico e Logs**: Mant√©m registro completo de todas as opera√ß√µes

## üõ†Ô∏è Tecnologias

- **Python 3.11**
- **FastAPI** - Framework web moderno e r√°pido
- **SQLAlchemy 2** + **Alembic** - ORM e migra√ß√µes de banco
- **PostgreSQL** - Banco de dados principal
- **Playwright** - Scraping de conte√∫do din√¢mico
- **OpenAI GPT-4o** - Extra√ß√£o inteligente de dados
- **Docker** + **Docker Compose** - Containeriza√ß√£o
- **Pytest** - Testes automatizados

## üìã Pr√©-requisitos

- Docker e Docker Compose
- Arquivo `companies.csv` com as empresas (ver formato abaixo)
- Chave API OpenAI (j√° configurada no projeto)

## üîß Instala√ß√£o e Execu√ß√£o

### 1. Clone o reposit√≥rio
```bash
git clone <repo-url>
cd aum-scraper-backend
```

### 2. Prepare o arquivo CSV
Crie um arquivo `companies.csv` na raiz do projeto com o formato:
```csv
name,url_site,url_linkedin,url_instagram,url_x
Empresa A,https://empresaa.com.br,https://linkedin.com/company/empresaa,https://instagram.com/empresaa,https://x.com/empresaa
Empresa B,https://empresab.com.br,https://linkedin.com/company/empresab,,
```

### 3. Execute com Docker Compose
```bash
# Construir e iniciar todos os servi√ßos
docker-compose up --build

# Ou executar em background
docker-compose up -d --build
```

### 4. Acesse a aplica√ß√£o
- **API Swagger**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **PostgreSQL**: localhost:5432
- **RabbitMQ Management**: http://localhost:15672 (guest/guest)

## üìö Uso da API

### Endpoints Principais

#### 1. Carregar Empresas do CSV
```bash
POST /api/v1/companies/load-from-csv
```

#### 2. Executar Pipeline Completo
```bash
POST /api/v1/scraping/pipeline/full
```

#### 3. Scraping de Empresa Espec√≠fica
```bash
POST /api/v1/scraping/company/{company_id}
```

#### 4. Exportar Resultados para Excel
```bash
GET /api/v1/scraping/export/excel
```

#### 5. Monitorar Uso de Budget
```bash
GET /api/v1/usage/today
GET /api/v1/usage/budget/status
```

### Exemplo de Uso Completo

1. **Carregar empresas do CSV**:
```bash
curl -X POST "http://localhost:8000/api/v1/companies/load-from-csv"
```

2. **Executar pipeline completo**:
```bash
curl -X POST "http://localhost:8000/api/v1/scraping/pipeline/full"
```

3. **Verificar resultados**:
```bash
curl "http://localhost:8000/api/v1/companies/stats/summary"
```

4. **Baixar relat√≥rio Excel**:
```bash
curl -o resultados.xlsx "http://localhost:8000/api/v1/scraping/export/excel"
```

## üóÉÔ∏è Estrutura do Banco de Dados

### Tabelas Principais

- **companies**: Dados das empresas (nome, URLs)
- **scrape_logs**: Logs de scraping (sucesso/falha, conte√∫do)
- **aum_snapshots**: Snapshots de AUM extra√≠dos
- **usage**: Registro de uso da API OpenAI

## üí∞ Controle de Budget

O sistema monitora automaticamente os gastos da API OpenAI:

- **Budget Di√°rio**: $50 USD (configur√°vel)
- **Alerta**: 80% do budget utilizado
- **Bloqueio**: Impede execu√ß√£o quando budget esgotado
- **Estimativa**: Calcula custos antes da execu√ß√£o

## üß™ Testes

Execute os testes automatizados:

```bash
# Dentro do container
docker-compose exec backend pytest tests/ -v --cov=app --cov-report=html

# Ou localmente (ap√≥s instalar depend√™ncias)
pytest tests/ -v --cov=app --cov-report=html
```

## üìä Monitoramento

### M√©tricas Dispon√≠veis

- Uso di√°rio de tokens e custos
- Taxa de sucesso do scraping
- Hist√≥rico de AUM por empresa
- Performance por fonte de dados

### Logs

Os logs s√£o armazenados em `/app/logs` dentro do container e podem ser acessados via:

```bash
docker-compose logs backend
```

## üîí Seguran√ßa e Limites

- **Rate Limiting**: M√°ximo 3 requests simult√¢neas
- **Delay entre Requests**: 1 segundo entre cada scraping
- **Timeout**: 30 segundos por request
- **Valida√ß√£o**: Todos os inputs s√£o validados com Pydantic

## üêõ Troubleshooting

### Problemas Comuns

1. **Erro de conex√£o com PostgreSQL**:
   - Verifique se o container do banco est√° rodando
   - Aguarde o healthcheck passar

2. **Budget esgotado**:
   - Monitore via `/api/v1/usage/today`
   - Ajuste o budget em `app/core/config.py`

3. **Scraping bloqueado**:
   - Use `use_playwright=true` para sites din√¢micos
   - Verifique logs em `/api/v1/scraping/status`

4. **Arquivo CSV n√£o encontrado**:
   - Verifique se est√° na raiz do projeto
   - Confirme o formato das colunas

## üìà Melhorias Futuras

- [ ] Integra√ß√£o com APIs de not√≠cias
- [ ] Scheduler para execu√ß√£o autom√°tica
- [ ] Dashboard web para visualiza√ß√£o
- [ ] Cache Redis para otimiza√ß√£o
- [ ] Alertas por email/Slack
- [ ] API rate limiting mais sofisticado

## üìù Licen√ßa

Este projeto foi desenvolvido como case t√©cnico e est√° dispon√≠vel para uso conforme especificado.

## ü§ù Suporte

Para d√∫vidas ou problemas:
1. Verifique os logs: `docker-compose logs backend`
2. Consulte a documenta√ß√£o da API: http://localhost:8000/docs
3. Execute os testes para validar o ambiente